import asyncio
import os
import re
import random
from datetime import datetime
from urllib.parse import urljoin

from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from bs4 import BeautifulSoup

# ====================================================
# üîß C·∫§U H√åNH
# ====================================================
INPUT_FILE = "input.txt"   # file ch·ª©a m√£ HTML copy t·ª´ trang
RATE_LIMIT = 10            # request m·ªói ph√∫t (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh b·ªã ch·∫∑n IP)
DELAY = 60 / RATE_LIMIT    # th·ªùi gian delay gi·ªØa c√°c ch∆∞∆°ng
MAX_RETRIES = 5            # s·ªë l·∫ßn th·ª≠ l·∫°i khi l·ªói k·∫øt n·ªëi
OUTPUT_FOLDER = "output"   # n∆°i l∆∞u ch∆∞∆°ng
# ====================================================


def natural_key(name: str):
    """D√πng ƒë·ªÉ s·∫Øp x·∫øp ch∆∞∆°ng theo th·ª© t·ª± t·ª± nhi√™n (1, 2, 10 thay v√¨ 1, 10, 2)."""
    parts = re.split(r'(\d+)', name)
    return [int(p) if p.isdigit() else p.lower() for p in parts]


# ============================
# üï∑Ô∏è ƒê·ªçc danh s√°ch ch∆∞∆°ng t·ª´ file input.txt
# ============================
async def get_chapter_list_from_file():
    print("üìÇ ƒêang ƒë·ªçc danh s√°ch ch∆∞∆°ng t·ª´ file input.txt...")

    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(
            f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}! H√£y copy m√£ ngu·ªìn trang (Ctrl+U) r·ªìi d√°n v√†o file n√†y."
        )

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, "html.parser")

    links = soup.select("ul.list-chapter li a")

    chapters = []
    for link in links:
        title = link.get_text(strip=True)
        href = link.get("href")
        if href and title:
            full_url = urljoin("https://novelbin.com", href)
            chapters.append((title, full_url))

    if not chapters:
        raise Exception("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh s√°ch ch∆∞∆°ng! C√≥ th·ªÉ b·∫°n copy ch∆∞a h·∫øt m√£ HTML trang.")

    chapters.sort(key=lambda x: natural_key(x[0]))
    print(f"üìö T√¨m th·∫•y {len(chapters)} ch∆∞∆°ng trong file input.txt.")
    return chapters


# ============================
# üìñ C√†o n·ªôi dung t·ª´ng ch∆∞∆°ng
# ============================
async def crawl_chapter(crawler, title, url, retries=MAX_RETRIES):
    for attempt in range(1, retries + 1):
        print(f"üìò ƒêang c√†o: {title} (l·∫ßn {attempt})")
        try:
            run_config = CrawlerRunConfig(
                cache_mode="bypass",
                css_selector="div#chr-content, div.text-left",
                exclude_external_links=True
            )

            result = await crawler.arun(url=url, config=run_config, timeout_ms=60000)

            if result.success:
                os.makedirs(OUTPUT_FOLDER, exist_ok=True)
                safe_name = re.sub(r'[\\/*?:"<>|]', "_", title)
                file_name = f"{safe_name}.txt"
                file_path = os.path.join(OUTPUT_FOLDER, file_name)

                cleaned = re.sub(r'\s+', ' ', result.markdown.strip())

                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(cleaned)

                print(f"üíæ ƒê√£ l∆∞u {file_name}")
                return

            else:
                print(f"‚ö†Ô∏è L·ªói khi c√†o {title} ({result.status_code})")

        except Exception as e:
            print(f"‚ùå L·ªói khi c√†o {title}: {e}")

        if attempt < retries:
            wait_time = 8 + random.uniform(1, 5)
            print(f"üîÅ Th·ª≠ l·∫°i sau {wait_time:.1f} gi√¢y...")
            await asyncio.sleep(wait_time)

    print(f"üö´ B·ªè qua {title} sau {retries} l·∫ßn th·∫•t b·∫°i.")


# ============================
# üèÅ Main
# ============================
async def main():
    # Nh·∫≠p ch∆∞∆°ng ho·∫∑c URL b·∫Øt ƒë·∫ßu
    start_input = input("üî¢ Nh·∫≠p t√™n ch∆∞∆°ng b·∫Øt ƒë·∫ßu ho·∫∑c URL ch∆∞∆°ng: ").strip()

    browser_config = BrowserConfig(
        headless=True,
        user_agent=(
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        chapters = await get_chapter_list_from_file()

        # N·∫øu ng∆∞·ªùi d√πng nh·∫≠p URL ‚Üí t√¨m v·ªã tr√≠ URL trong danh s√°ch
        start_index = 0
        if start_input.startswith("http"):
            for i, (_, url) in enumerate(chapters):
                if start_input.strip() in url:
                    start_index = i
                    break
            else:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y URL trong danh s√°ch ch∆∞∆°ng, s·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu.")
        else:
            # N·∫øu nh·∫≠p t√™n ch∆∞∆°ng (vd: 123.1 ho·∫∑c 134)
            for i, (title, _) in enumerate(chapters):
                if re.search(re.escape(start_input), title, re.IGNORECASE):
                    start_index = i
                    break
            else:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ch∆∞∆°ng tr√πng kh·ªõp, s·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu.")

        # C·∫Øt danh s√°ch ƒë·ªÉ ch·ªâ l·∫•y t·ª´ ch∆∞∆°ng c·∫ßn b·∫Øt ƒë·∫ßu
        chapters = chapters[start_index:]
        print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o t·ª´ ch∆∞∆°ng: {chapters[0][0]} (th·ª© t·ª± {start_index + 1}/{len(chapters) + start_index})\n")

        total = len(chapters)
        for idx, (title, url) in enumerate(chapters, start=1):
            await crawl_chapter(crawler, title, url)

            if idx < total:
                delay_time = DELAY + random.uniform(0.5, 3.0)
                print(f"‚è≥ Ch·ªù {delay_time:.1f}s tr∆∞·ªõc khi ti·∫øp t·ª•c...")
                await asyncio.sleep(delay_time)

    print("üéâ Ho√†n t·∫•t to√†n b·ªô ch∆∞∆°ng!")


if __name__ == "__main__":
    asyncio.run(main())
